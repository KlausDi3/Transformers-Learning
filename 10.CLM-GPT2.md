# Causal Language Model (CLM) - GPT-2 Training Guide

## Part 1 — Model & Code Structure Overview

### 1) Model

**A causal language model (CLM) in the GPT-2 family.**

**Goal:** predict the next token given all previous tokens.

**High-level architecture:**

```
Tokens → Token Embedding + Positional Embedding
       → [Decoder Block × N]
           └─ Masked Self-Attention (causal mask: only look left)
           └─ MLP (feed-forward)
           └─ Residual + LayerNorm around sublayers
       → LM Head (linear to vocab, usually tied with embeddings)
       → Next-token logits
```

**Training objective:** minimize cross-entropy between predicted next-token distribution and the ground truth next token (per position).

### 2) How the code assembles this model end-to-end

Think of the notebook as a pipeline with clear stages:

#### (A) Data & tokenization

- Define `context_length=128`
- Use `AutoTokenizer` to split long texts into fixed-length chunks:
  - `truncation=True, max_length=128, return_overflowing_tokens=True, return_length=True`
- Keep only chunks whose length == 128 (uniform sequences)

#### (B) Dataset building

- Apply a `tokenize()` function over the whole dataset with `dataset.map(..., batched=True)`
- Drop original text columns and keep `input_ids` (and later `attention_mask`)

#### (C) Model config & initialization (from scratch)

- `AutoConfig.from_pretrained("gpt2")` as a template; set:
  - `vocab_size=len(tokenizer), n_ctx=128`
  - `bos_token_id, eos_token_id`
- `model = GPT2LMHeadModel(config)` → random init (no pretrained weights)

#### (D) Batching & padding

- GPT-2 has no PAD → set `tokenizer.pad_token = tokenizer.eos_token`
- Use `DataCollatorForLanguageModeling(tokenizer, mlm=False)` to:
  - pad sequences in a batch
  - provide labels for CLM (next-token prediction)

#### (E) Two training paths

**Trainer (high-level):** configure `TrainingArguments` (batch sizes, FP16, warmup, scheduler, eval every N steps, push_to_hub), then:

```python
trainer = Trainer(model=..., tokenizer=..., args=..., data_collator=..., 
                  train_dataset=..., eval_dataset=...)
trainer.train()
trainer.push_to_hub()
```

**Accelerate (manual loop, flexible):**

```python
accelerator = Accelerator(fp16=True)
model, optimizer, train_dl, eval_dl = accelerator.prepare(model, optimizer, train_dl, eval_dl)
for step, batch in train_dl:
    out = model(**batch) or custom loss → accelerator.backward(loss)
    (grad accumulation) → clip → optimizer.step() → lr_scheduler.step() → zero_grad()
    (periodic eval + save + push)
```

#### (F) Optimizer & parameter groups

**AdamW with selective weight decay:**
- decay most weights
- no decay for bias and LayerNorm.weight

#### (G) LR scheduler & warmup

Warmup first (e.g., 1000 steps), then linear/cosine decay via `get_scheduler(...)`

#### (H) Evaluation & perplexity

Validation loop computes mean loss; **perplexity = exp(loss)** (lower is better)

#### (I) Saving & Hub

- **With Trainer:** `push_to_hub=True` or `trainer.push_to_hub()`
- **With Accelerate:** `unwrap_model → save_pretrained → repo.push_to_hub(...)`

### Code flow (mental map)

```
Raw text
  → Tokenizer (128-chunks)
  → Dataset.map (fixed-length examples)
  → DataCollator (pad + labels)
  → Model (GPT-2-like, from config)
  → Optimizer (AdamW, param groups)
  → Scheduler (warmup + decay)
  → Train (Trainer or Accelerate loop)
  → Evaluate (loss → PPL)
  → Save / Push to Hub
  → (Optional) pipeline("text-generation") for inference
```

## Part 2 — Key Points to Remember

### Important Concepts

- **Dict unpacking:** `model(**batch)` ≡ `model(input_ids=..., attention_mask=..., labels=...)`
- **Never do** `model(**outputs)` (those are model outputs, not inputs)
- **Batch shapes:** typically `input_ids / attention_mask / labels → [batch_size, 128]`

### GPT-2 Specific

- **Why set `pad_token = eos_token` for GPT-2:** GPT-2 lacks a PAD token; we use EOS as a safe filler. Attention masking ensures pads are ignored.

### Training Techniques

- **Gradient accumulation:** divide loss by `grad_accum_steps`; call backward each mini-step; update weights every N steps → simulates larger effective batch
- **Gradient clipping:** `clip_grad_norm_(..., 1.0)` to avoid gradient explosion
- **Mixed precision (FP16):** `fp16=True` (Trainer) or `Accelerator(fp16=True)` → faster & less memory; use `accelerator.backward(loss)`
- **Scheduler + warmup:** start LR small, warm up to base LR, then decay (linear or cosine). Call `lr_scheduler.step()` after `optimizer.step()`

### Custom Loss (key-token weighted)

1. Shift labels (next-token)
2. Compute per-token CE → average per sample
3. Weight samples by count of "key tokens" (× alpha)
4. Emphasizes learning on important patterns

### Evaluation & Deployment

- **Evaluation metric:** Perplexity = exp(val_loss). Lower is better; gather losses across devices before averaging
- **Push to Hub:** 
  - Trainer—one line
  - Accelerate—`unwrap_model → save_pretrained → repo.push_to_hub`

### Common Pitfalls

⚠️ **Watch out for:**
- Forgetting `attention_mask` when padding
- `vocab_size` not matching tokenizer size
- Passing wrong keys to `model.forward` (must match signature)
- **Logging LR:** `optimizer.param_groups[0]["lr"]`
