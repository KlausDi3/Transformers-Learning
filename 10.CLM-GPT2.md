# Causal Language Model (CLM) - GPT-2 Training Guide

## Part 1 — Model & Code Structure Overview

### 1) Model

**A causal language model (CLM) in the GPT-2 family.**

**Goal:** predict the next token given all previous tokens.

**High-level architecture:**

```
Tokens → Token Embedding + Positional Embedding
       → [Decoder Block × N]
           └─ Masked Self-Attention (causal mask: only look left)
           └─ MLP (feed-forward)
           └─ Residual + LayerNorm around sublayers
       → LM Head (linear to vocab, usually tied with embeddings)
       → Next-token logits
```

**Training objective:** minimize cross-entropy between predicted next-token distribution and the ground truth next token (per position).

### 2) How the code assembles this model end-to-end

Think of the notebook as a pipeline with clear stages:

#### (A) Data & tokenization

- Define `context_length=128`
- Use `AutoTokenizer` to split long texts into fixed-length chunks:
  - `truncation=True, max_length=128, return_overflowing_tokens=True, return_length=True`
- Keep only chunks whose length == 128 (uniform sequences)

#### (B) Dataset building

- Apply a `tokenize()` function over the whole dataset with `dataset.map(..., batched=True)`
- Drop original text columns and keep `input_ids` (and later `attention_mask`)

#### (C) Model config & initialization (from scratch)

- `AutoConfig.from_pretrained("gpt2")` as a template; set:
  - `vocab_size=len(tokenizer), n_ctx=128`
  - `bos_token_id, eos_token_id`
- `model = GPT2LMHeadModel(config)` → random init (no pretrained weights)

#### (D) Batching & padding

- GPT-2 has no PAD → set `tokenizer.pad_token = tokenizer.eos_token`
- Use `DataCollatorForLanguageModeling(tokenizer, mlm=False)` to:
  - pad sequences in a batch
  - provide labels for CLM (next-token prediction)

#### (E) Two training paths

**Trainer (high-level):** configure `TrainingArguments` (batch sizes, FP16, warmup, scheduler, eval every N steps, push_to_hub), then:

```python
trainer = Trainer(model=..., tokenizer=..., args=..., data_collator=..., 
                  train_dataset=..., eval_dataset=...)
trainer.train()
trainer.push_to_hub()
```

**Accelerate (manual loop, flexible):**

```python
accelerator = Accelerator(fp16=True)
model, optimizer, train_dl, eval_dl = accelerator.prepare(model, optimizer, train_dl, eval_dl)
for step, batch in train_dl:
    out = model(**batch) or custom loss → accelerator.backward(loss)
    (grad accumulation) → clip → optimizer.step() → lr_scheduler.step() → zero_grad()
    (periodic eval + save + push)
```

#### (F) Optimizer & parameter groups

**AdamW with selective weight decay:**
- decay most weights
- no decay for bias and LayerNorm.weight

#### (G) LR scheduler & warmup

Warmup first (e.g., 1000 steps), then linear/cosine decay via `get_scheduler(...)`

#### (H) Evaluation & perplexity

Validation loop computes mean loss; **perplexity = exp(loss)** (lower is better)

#### (I) Saving & Hub

- **With Trainer:** `push_to_hub=True` or `trainer.push_to_hub()`
- **With Accelerate:** `unwrap_model → save_pretrained → repo.push_to_hub(...)`

### Code flow (mental map)

```
Raw text
  → Tokenizer (128-chunks)
  → Dataset.map (fixed-length examples)
  → DataCollator (pad + labels)
  → Model (GPT-2-like, from config)
  → Optimizer (AdamW, param groups)
  → Scheduler (warmup + decay)
  → Train (Trainer or Accelerate loop)
  → Evaluate (loss → PPL)
  → Save / Push to Hub
  → (Optional) pipeline("text-generation") for inference
```

## Part 2 — Key Points to Remember

### Important Concepts

- **Dict unpacking:** `model(**batch)` ≡ `model(input_ids=..., attention_mask=..., labels=...)`
- **Never do** `model(**outputs)` (those are model outputs, not inputs)
- **Batch shapes:** typically `input_ids / attention_mask / labels → [batch_size, 128]`

### GPT-2 Specific

- **Why set `pad_token = eos_token` for GPT-2:** GPT-2 lacks a PAD token; we use EOS as a safe filler. Attention masking ensures pads are ignored.

### Training Techniques

- **Gradient accumulation:** divide loss by `grad_accum_steps`; call backward each mini-step; update weights every N steps → simulates larger effective batch
- **Gradient clipping:** `clip_grad_norm_(..., 1.0)` to avoid gradient explosion
- **Mixed precision (FP16):** `fp16=True` (Trainer) or `Accelerator(fp16=True)` → faster & less memory; use `accelerator.backward(loss)`
- **Scheduler + warmup:** start LR small, warm up to base LR, then decay (linear or cosine). Call `lr_scheduler.step()` after `optimizer.step()`

### Custom Loss (key-token weighted)

1. Shift labels (next-token)
2. Compute per-token CE → average per sample
3. Weight samples by count of "key tokens" (× alpha)
4. Emphasizes learning on important patterns

### Evaluation & Deployment

- **Evaluation metric:** Perplexity = exp(val_loss). Lower is better; gather losses across devices before averaging
- **Push to Hub:** 
  - Trainer—one line
  - Accelerate—`unwrap_model → save_pretrained → repo.push_to_hub`

### Common Pitfalls

⚠️ **Watch out for:**
- Forgetting `attention_mask` when padding
- `vocab_size` not matching tokenizer size
- Passing wrong keys to `model.forward` (must match signature)
- **Logging LR:** `optimizer.param_groups[0]["lr"]`

## Part 3 — Comprehensive Batch Processing Guide

### 3.1 Data Preparation: From Raw Data to Tokenized Data

#### Raw Dataset Structure

```python
raw_datasets = DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

**Sample data point:**
```python
{
    'repo_name': 'kmike/scikit-learn',
    'path': 'sklearn/utils/__init__.py',
    'content': '"""The :mod:`sklearn.utils` module..."""\nimport numpy...',
    'license': 'bsd-3-clause',
    ...
}
```

#### Tokenization Process

```python
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=128,  # context_length
        return_overflowing_tokens=True,
        return_length=True,
    )
    # Keep only chunks with exactly 128 tokens
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == 128:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}
```

**Key Points:**
- Long code files are split into multiple 128-token chunks
- Chunks shorter than 128 tokens are discarded (ensuring fixed length)
- Each chunk becomes an independent training sample

**Example:**
```python
# Original: 2 code files
# After tokenization:
Input IDs length: 34  # Generated 34 chunks
Input chunk lengths: [128, 128, ..., 117, ..., 41]  # Mostly 128, some shorter
Chunk mapping: [0, 0, ..., 0, 1, 1, ..., 1]  # First 20 from file 1, next 14 from file 2

# After filtering (keeping only 128-length chunks):
# File 1 → 19 samples (discard the 117-token chunk)
# File 2 → 13 samples (discard the 41-token chunk)
# Total: 32 fixed-length samples
```

#### Processed Dataset

```python
tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=...)

# Result:
DatasetDict({
    train: Dataset({
        features: ['input_ids'],  # Only input_ids remains
        num_rows: 16702061        # Much larger than original 606720 (due to splitting)
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

**Each sample is now:**
```python
{
    'input_ids': [50256, 2235, 8776, ..., 198]  # Fixed length of 128
}
```

### 3.2 Batch Creation: Data Collator's Role

#### Data Collator Initialization

```python
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token  # Use EOS as padding token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

**Parameters:**
- `mlm=False`: No Masked Language Modeling (since this is GPT-style causal language model)
- **Purpose:** Organize multiple samples into a batch and automatically generate labels and attention_mask

#### Manual Batch Creation (Demonstration)

```python
# Take 5 samples from the dataset
batch = data_collator([tokenized_datasets["train"][i] for i in range(5)])

# Output:
for key in batch:
    print(f"{key} shape: {batch[key].shape}")
```

**Output:**
```
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

### 3.3 Batch Structure: What's Inside

#### Three Components of a Batch

A batch is a **dictionary** containing three tensors:

```python
batch = {
    'input_ids': tensor([[...]]),      # shape: [batch_size, seq_length]
    'attention_mask': tensor([[...]]), # shape: [batch_size, seq_length]
    'labels': tensor([[...]])          # shape: [batch_size, seq_length]
}
```

#### Detailed Structure Example

Assuming batch_size=5, seq_length=128:

```python
batch = {
    'input_ids': tensor([
        [50256, 2235, 8776, ..., 198],  # Sample 1's token IDs
        [50256, 1234, 5678, ..., 198],  # Sample 2's token IDs
        [50256, 9012, 3456, ..., 198],  # Sample 3
        [50256, 7890, 1234, ..., 198],  # Sample 4
        [50256, 5678, 9012, ..., 198],  # Sample 5
    ]),  # shape: [5, 128]

    'attention_mask': tensor([
        [1, 1, 1, ..., 1],  # Sample 1: all 1s (indicating real tokens)
        [1, 1, 1, ..., 1],  # Sample 2
        [1, 1, 1, ..., 1],  # Sample 3
        [1, 1, 1, ..., 1],  # Sample 4
        [1, 1, 1, ..., 1],  # Sample 5
    ]),  # shape: [5, 128]

    'labels': tensor([
        [50256, 2235, 8776, ..., 198],  # Identical to input_ids
        [50256, 1234, 5678, ..., 198],
        [50256, 9012, 3456, ..., 198],
        [50256, 7890, 1234, ..., 198],
        [50256, 5678, 9012, ..., 198],
    ])  # shape: [5, 128]
}
```

#### Field Meanings

| Field | Meaning | Shape | Purpose |
|-------|---------|-------|---------|
| `input_ids` | Token ID sequence | `[5, 128]` | Model input |
| `attention_mask` | Attention mask | `[5, 128]` | Mark which positions are real tokens (1) or padding (0) |
| `labels` | Training labels | `[5, 128]` | Used for loss calculation (same as input_ids in causal language models) |

**Why are labels and input_ids the same?**

In causal language models (GPT):
- Input: `[token_0, token_1, token_2, ..., token_127]`
- Label: `[token_0, token_1, token_2, ..., token_127]`
- Model prediction: Use `token_0...token_n-1` to predict `token_n`
- Internal position shifting (shift) is handled automatically

### 3.4 Batch into Model: Forward Pass Process

#### Model Call Methods

```python
# Method 1: Dictionary unpacking (recommended)
output = model(**batch)

# Equivalent to Method 2:
output = model(
    input_ids=batch['input_ids'],
    attention_mask=batch['attention_mask'],
    labels=batch['labels']
)

# Equivalent to Method 3:
output = model.forward(
    input_ids=batch['input_ids'],
    attention_mask=batch['attention_mask'],
    labels=batch['labels']
)
```

#### Model Internal Processing Flow

```python
# GPT2LMHeadModel forward method pseudocode:

def forward(self, input_ids, attention_mask=None, labels=None):
    # 1. Embedding layers
    # input_ids: [5, 128] → embeddings: [5, 128, 768]
    inputs_embeds = self.transformer.wte(input_ids)  # Word Token Embedding
    position_embeds = self.transformer.wpe(position_ids)  # Position Embedding
    hidden_states = inputs_embeds + position_embeds

    # 2. Through 12 Transformer Blocks
    # hidden_states: [5, 128, 768]
    for block in self.transformer.h:
        hidden_states = block(hidden_states, attention_mask)

    # 3. Layer Normalization
    hidden_states = self.transformer.ln_f(hidden_states)

    # 4. Language Model Head (linear layer)
    # hidden_states: [5, 128, 768] → logits: [5, 128, 50000]
    logits = self.lm_head(hidden_states)

    # 5. Loss calculation (if labels provided)
    if labels is not None:
        # Shift: use first 127 tokens to predict next 127 tokens
        shift_logits = logits[..., :-1, :].contiguous()  # [5, 127, 50000]
        shift_labels = labels[..., 1:].contiguous()      # [5, 127]

        # Calculate cross-entropy loss
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(
            shift_logits.view(-1, 50000),  # [5*127, 50000]
            shift_labels.view(-1)           # [5*127]
        )

    return CausalLMOutput(
        loss=loss,        # scalar
        logits=logits,    # [5, 128, 50000]
        ...
    )
```

#### Actual Output Example

```python
output = model(**batch)

print(output.loss)    # tensor(10.9196)
print(output.logits.shape)  # torch.Size([5, 128, 50000])
```

**Output structure:**
```python
CausalLMOutputWithCrossAttentions(
    loss=tensor(10.9196, grad_fn=<NllLossBackward0>),  # Loss value
    logits=tensor([[[...]]]),  # [5, 128, 50000] - probability distribution for each position over 50000 words
    past_key_values=...,       # Cached key-value (for generation acceleration)
    hidden_states=None,
    attentions=None
)
```

#### Tensor Shape Changes Tracking

| Stage | Operation | Input Shape | Output Shape |
|-------|-----------|-------------|--------------|
| 0 | Batch input | - | `[5, 128]` |
| 1 | Word Embedding | `[5, 128]` | `[5, 128, 768]` |
| 2 | Position Embedding | `[5, 128]` | `[5, 128, 768]` |
| 3 | 12 × Transformer Block | `[5, 128, 768]` | `[5, 128, 768]` |
| 4 | Layer Norm | `[5, 128, 768]` | `[5, 128, 768]` |
| 5 | LM Head | `[5, 128, 768]` | `[5, 128, 50000]` |
| 6 | Loss calculation | `[5, 128, 50000]` + `[5, 128]` | scalar |

### 3.5 Complete Process Flow Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. Raw Dataset                                                   │
│    raw_datasets["train"][0]                                      │
│    {                                                             │
│      'content': "import numpy as np\nimport pandas...",         │
│      'repo_name': 'kmike/scikit-learn',                         │
│      ...                                                         │
│    }                                                             │
└─────────────────────────────────────────────────────────────────┘
                            ↓
                    tokenize() function
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│ 2. Tokenized Dataset                                             │
│    tokenized_datasets["train"][0]                                │
│    {                                                             │
│      'input_ids': [50256, 2235, 8776, ..., 198]  # length 128   │
│    }                                                             │
└─────────────────────────────────────────────────────────────────┘
                            ↓
                  Take 5 samples + DataCollator
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│ 3. Batch (dictionary)                                            │
│    batch = {                                                     │
│      'input_ids':      tensor([[...]]),  # [5, 128]            │
│      'attention_mask': tensor([[...]]),  # [5, 128]            │
│      'labels':         tensor([[...]])   # [5, 128]            │
│    }                                                             │
└─────────────────────────────────────────────────────────────────┘
                            ↓
                       model(**batch)
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│ 4. Model Forward Pass                                            │
│                                                                  │
│  input_ids [5, 128]                                             │
│       ↓                                                          │
│  Word Embedding + Position Embedding                            │
│       ↓                                                          │
│  [5, 128, 768]                                                  │
│       ↓                                                          │
│  12 × GPT2Block (Self-Attention + MLP)                         │
│       ↓                                                          │
│  [5, 128, 768]                                                  │
│       ↓                                                          │
│  Layer Norm                                                     │
│       ↓                                                          │
│  LM Head (Linear: 768 → 50000)                                 │
│       ↓                                                          │
│  logits [5, 128, 50000]                                        │
│       ↓                                                          │
│  CrossEntropyLoss(logits, labels)                              │
│       ↓                                                          │
│  loss (scalar)                                                  │
└─────────────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────────────┐
│ 5. Model Output                                                  │
│    output = {                                                    │
│      'loss': tensor(10.9196),                                   │
│      'logits': tensor([[[...]]]),  # [5, 128, 50000]          │
│      ...                                                         │
│    }                                                             │
└─────────────────────────────────────────────────────────────────┘
                            ↓
                    Backpropagation + Parameter Update
```

### 3.6 Real Training Batches

In actual training, handled automatically through Trainer:

```python
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        per_device_train_batch_size=32,  # 32 samples per GPU
        gradient_accumulation_steps=8,   # Accumulate 8 batches before update
        ...
    ),
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
)

trainer.train()
```

#### Real Batch Flow

1. **DataLoader automatic sampling**
   - Randomly sample 32 samples from `tokenized_datasets["train"]`

2. **Data Collator automatic organization**
   ```python
   batch = {
       'input_ids': [32, 128],
       'attention_mask': [32, 128],
       'labels': [32, 128]
   }
   ```

3. **Feed to model**
   ```python
   output = model(**batch)
   loss = output.loss
   ```

4. **Gradient accumulation**
   - Accumulate gradients from 8 batches
   - Effective batch size = 32 × 8 = 256

5. **Parameter update**
   ```python
   optimizer.step()      # Update parameters
   optimizer.zero_grad() # Clear gradients
   ```

### 3.7 Key Knowledge Points Summary

#### Batch Size Meaning

```python
per_device_train_batch_size = 32
```
- **Meaning:** Take 32 samples from dataset each time (not 32 tokens)
- **Each sample:** Already fixed length of 128 tokens
- **Total batch tokens:** 32 × 128 = 4096 tokens

#### Data Flow Path

```
Raw code files (strings)
    ↓ tokenize
Token ID lists [variable length]
    ↓ split into 128-token chunks + filter
Fixed-length samples {'input_ids': [128]}
    ↓ DataLoader sample 32
List of 32 samples
    ↓ DataCollator organize
Batch dictionary {
    'input_ids': [32, 128],
    'attention_mask': [32, 128],
    'labels': [32, 128]
}
    ↓ model(**batch)
Output {
    'loss': scalar,
    'logits': [32, 128, 50000]
}
    ↓
Backpropagation → Update parameters
```

#### Why Fixed Length?

1. **GPU parallel computation:** Requires all samples to have same length
2. **Avoid padding waste:** Discarding insufficient length parts is more efficient than padding
3. **Simplify training:** No need to handle dynamic length complexity

#### Dictionary Unpacking (`**`) Principle

```python
batch = {'input_ids': ..., 'attention_mask': ..., 'labels': ...}

# These two approaches are completely equivalent:
model(**batch)
model(input_ids=batch['input_ids'],
      attention_mask=batch['attention_mask'],
      labels=batch['labels'])
```

`**` expands dictionary key-value pairs into **keyword arguments**.

### 3.8 Common Questions FAQ

#### Q1: Why are labels and input_ids the same?

**A:** Causal language model (GPT) training objective is "predict next token":
- Input: `[token_0, token_1, ..., token_126]`
- Label: `[token_1, token_2, ..., token_127]`
- Although they look the same, the model internally handles position shifting

#### Q2: Why is attention_mask all 1s?

**A:** Because we filtered out insufficient length samples, all samples are real 128 tokens, no padding needed, so attention_mask is all 1s.

If there was padding, it would look like:
```python
input_ids      = [50256, 2235, 8776, ..., 50256, 50256, 50256]  # padding at end
attention_mask = [1,     1,    1,    ..., 0,     0,     0    ]  # padding positions are 0
```

#### Q3: Is batch_size=32 referring to 32 samples or 32 tokens?

**A:** 32 **samples**, each sample has 128 tokens, total 32 × 128 = 4096 tokens.

#### Q4: What does gradient accumulation mean?

**A:**
```python
gradient_accumulation_steps = 8
```
- Not updating parameters after every batch
- Instead, accumulate gradients from 8 batches before updating once
- **Effective batch size** = 32 × 8 = 256
- Purpose: Simulate large batch size when memory is limited

### 3.9 Code Practice: Manual Batch Creation and Processing

```python
# 1. Prepare data
from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling
from datasets import load_dataset

# Load data
tokenized_datasets = load_dataset("huggingface-course/codeparrot-ds-train", split="train")

# Initialize
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# 2. Manually create batch
batch = data_collator([tokenized_datasets[i] for i in range(5)])

print("Batch keys:", batch.keys())
print("Batch shapes:")
for key, value in batch.items():
    print(f"  {key}: {value.shape}")

# 3. View specific content
print("\nFirst 10 token IDs of first sample:")
print(batch['input_ids'][0][:10])

print("\nFirst 10 attention mask values of first sample:")
print(batch['attention_mask'][0][:10])

# 4. Feed to model
model = GPT2LMHeadModel.from_pretrained("gpt2")
output = model(**batch)

print(f"\nLoss: {output.loss.item():.4f}")
print(f"Logits shape: {output.logits.shape}")

# 5. Decode and view content
print("\nDecoded code from first sample:")
decoded = tokenizer.decode(batch['input_ids'][0])
print(decoded[:200])  # Show only first 200 characters
```

### 3.10 Summary

#### Core Points

1. **Batch is a dictionary:** Contains `input_ids`, `attention_mask`, `labels`
2. **Shape is `[batch_size, seq_length]`:** e.g., `[32, 128]`
3. **DataCollator automatically organizes:** From sample list to batch dictionary
4. **`**batch` unpacking for parameters:** Convenient and commonly used model calling method
5. **Model internally handles automatically:** embedding → transformer → logits → loss

#### Data Flow Summary

```
Raw text → tokenize → split into 128-token chunks → filter
    ↓
Fixed-length samples {'input_ids': [128]}
    ↓
DataLoader sample 32
    ↓
DataCollator organize into batch {
    'input_ids': [32, 128],
    'attention_mask': [32, 128],
    'labels': [32, 128]
}
    ↓
model(**batch) → {
    'loss': scalar,
    'logits': [32, 128, 50000]
}
    ↓
Backpropagation → Update parameters
```

**Learning Recommendations:**
1. First understand single sample structure
2. Then understand how batch combines multiple samples
3. Finally understand how model processes batch
4. Practice running code, print shapes at each step
