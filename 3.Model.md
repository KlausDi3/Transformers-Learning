# HuggingFace Transformers Study Notes  
**Topic: Model Basics and Text Classification Practice**  

---

## 1. Basic Concepts

### 1.1 Tokenizer  
- Converts natural language text into token sequences and corresponding IDs.  
- Handles tokenization, adding special tokens ([CLS], [SEP]), padding, and truncation.  
- Ensures inputs are in the correct format for the model.  

### 1.2 Model  
- A neural network architecture and weights pre-trained on large-scale corpora.  
- Loaded via `from_pretrained()` and can be directly applied to downstream tasks (classification, generation, QA, etc.).  
- Options include base models (e.g., `BertModel`) or models with task-specific heads (e.g., `BertForSequenceClassification`).  

### 1.3 logits  
- Raw output scores from the model before softmax.  
- Commonly used in classification tasks for argmax prediction or probability calculation.  

### 1.4 Pipeline  
- A wrapper provided by Transformers for simplified inference.  
- Suitable for quick experiments, but less flexible for custom training.  

---

## 2. Basic Usage of Models

### 2.1 Model Loading and Initialization

```python
from transformers import AutoModel, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")
```

`from_pretrained` loads models from remote repositories or local files.

**Common arguments:**
- `cache_dir`: specify cache directory
- `revision`: select specific model version
- `num_labels`: define number of labels for classification tasks

### 2.2 Preparing Input Data
Single text encoding example:

```python
inputs = tokenizer("this is a test", return_tensors="pt",
                   padding=True, truncation=True, max_length=128)
```

**Output includes:**
- `input_ids`: token ID sequence
- `attention_mask`: distinguish real tokens from padding
- `token_type_ids`: required for sentence pair tasks (e.g., NLI)

**Length handling:**
- `max_length=128` limits input length
- `padding=True` ensures equal length within a batch
- `truncation=True` avoids exceeding model's maximum length (e.g., 512 tokens for BERT)

### 2.3 Forward Pass and Output
```python
import torch
with torch.no_grad():
    outputs = model(**inputs)
```

**Output contents:**
- Base model (e.g., `BertModel`) → `last_hidden_state`, `pooler_output`
- Classification model (e.g., `BertForSequenceClassification`) → `logits`

### 2.4 Post-processing
```python
logits = outputs.logits
probs = torch.softmax(logits, dim=-1)
preds = torch.argmax(probs, dim=-1)
```
- `softmax` → probability distribution
- `argmax` → predicted class

### 2.5 Common Issues and Notes

| Issue | Description | Solution |
|-------|-------------|----------|
| Model and tokenizer mismatch | Different vocabularies or encodings | Always use the same model name for both |
| Input too long | Exceeds model maximum length | Use `truncation=True` |
| Input too short | Uneven batch sizes | Use `padding` |
| Device mismatch | Tensors on CPU, model on GPU | Use `.to(device)` for consistency |
| Misunderstanding outputs | Confusing logits and hidden states | Check official docs and output structure |

---

## 3. Text Classification Practice

### 3.1 Workflow
1. Prepare dataset (train / validation / test)
2. Load tokenizer and model (`AutoModelForSequenceClassification`)
3. Encode text data into `input_ids` and `attention_mask`
4. Build Dataset / DataLoader or use `datasets` library
5. Configure optimizer (AdamW) and scheduler
6. Train using `Trainer` or custom training loop
7. Evaluate on validation set and save the model

### 3.2 Core Code Structure
```python
from transformers import (AutoTokenizer, AutoModelForSequenceClassification,
                          Trainer, TrainingArguments)
import datasets

# 1. Load dataset
dataset = datasets.load_dataset("imdb")

# 2. Initialize tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# 3. Tokenization function
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 4. Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# 5. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

# 6. Train + evaluate
trainer.train()
trainer.evaluate()
```

### 3.3 Parameter Explanation

| Parameter | Meaning | Typical Values |
|-----------|---------|----------------|
| `num_labels` | Number of classification labels | 2 (binary), N (multi-class) |
| `max_length` | Maximum input length | 128 / 256 (BERT max 512) |
| `learning_rate` | Learning rate | 1e-5 ~ 5e-5 |
| `per_device_train_batch_size` | Training batch size per device | 8 / 16 / 32 |
| `num_train_epochs` | Number of training epochs | 3 ~ 5 |
| `weight_decay` | Weight decay for regularization | 0.01 |

### 3.4 Optimization and Tips
- **Freeze layers**: Train only top layers and classification head to save memory
- **Adjust dropout**: Control overfitting
- **Learning rate scheduling**: Warm-up + linear decay improves convergence
- **Handle class imbalance**: Weighted loss, oversampling, undersampling
- **Early stopping**: Stop when validation metrics plateau

### 3.5 Common Issues and Solutions

| Issue | Cause | Solution |
|-------|--------|----------|
| GPU out of memory | Large batch size / long max_length | Reduce batch size, shorten max_length, use smaller models (e.g., DistilBERT) |
| Overfitting | Small dataset / too many epochs | Use dropout, weight decay, early stopping |
| Slow convergence | Poor learning rate | Adjust LR, add warm-up, use scheduler |
| Class imbalance | Skewed dataset distribution | Weighted loss, sampling, focal loss |

---

## 4. Overall Process Summary

### 4.1 Understand model structure
- **Tokenizer**: text → token IDs
- **Model**: inputs → logits / hidden states

### 4.2 Inference workflow
- Encode input → forward pass → logits → softmax/argmax

### 4.3 Training framework
- Dataset preparation → Tokenization → DataLoader
- Model + Trainer/custom loop → training + validation

### 4.4 Optimization techniques
- Learning rate scheduling, regularization, early stopping
- Handle class imbalance and GPU memory limits

### 4.5 Practical applications
- Sentiment classification (IMDB)
- News classification (AG News)
- Other downstream tasks (QA, text matching, emotion detection, etc.)
